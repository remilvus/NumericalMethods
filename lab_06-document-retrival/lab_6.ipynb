{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyszukiwarka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"OANC-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do przetestowania wyszukiwarki dokumentów zostanie wykorzystany korpus tekstów OANC (Open American National Corpus) zawierający prawie 9000 elementów.\n",
    "http://www.anc.org/data/oanc/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_txt(source, to):\n",
    "    for name in os.listdir(source):\n",
    "        if name[-3:]==\"txt\":\n",
    "            target = to+\"/\"+name\n",
    "            os.rename(source+\"/\"+name, target)\n",
    "        if os.path.isdir(source+\"/\"+name):\n",
    "            move_txt(source+\"/\"+name, to)\n",
    "move_txt(source=\"data\", to=DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Określanie słów kluczowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako zbiór słów kluczowych zostanie wykorzystana część zbioru będącego unią wszystkich słów występujących we wszystkich elementach korpusu. Słowa stanowiące klucze zostaną wybrane na podstawie częstotliwości ich występowania w tekście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8806/8806 [03:16<00:00, 44.85it/s]\n"
     ]
    }
   ],
   "source": [
    "key_words = set()\n",
    "documents = [name for name in os.listdir(DATA_PATH)]#[:1500]\n",
    "bag_of_words = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "def split(text):\n",
    "    return re.findall(r\"[\\w]+\", text)\n",
    "\n",
    "for filename in tqdm(documents, position=0):\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    words = split(text)\n",
    "    for word in words:\n",
    "        bag_of_words[filename][word] += 1\n",
    "    key_words |= set(words)\n",
    "key_words = list(key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = defaultdict(lambda: 0)\n",
    "for doc_words in bag_of_words.values():\n",
    "    for key, val in doc_words.items():\n",
    "        word_count[key] += val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163263"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words used: 10032\n",
      "% of words discarded: 93.85531320629904\n"
     ]
    }
   ],
   "source": [
    "print(\"Words used:\", sum([1 for k, v in word_count.items() if v>=100]))\n",
    "print(\"% of words discarded:\",100 * sum([1 for k, v in word_count.items() if v<100]) / len(key_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako klucze zostaną wykorzystane tylko słowa pojawiające się łącznie we wszystkich tekstach co najmniej 100 razy. Zbiór słów kluczowych liczy więc 10032 słowa, co oznacza, że odrzucone zostaje około 94% wszystkich słów. Zostaną one wszystkie zakodowane pod tym samym indeksem w  wektorze bag-of-words.  \n",
    "Odrzucenie tak dużej ilości słów jest głównie spowodowane ograniczeniami sprzętowymi - wykonywane w dalszej części obliczenia mają duże wymagania pamięciowe, które są proporcjonalne m.in. do ilośći słów kluczowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163263/163263 [00:00<00:00, 1389226.31it/s]\n"
     ]
    }
   ],
   "source": [
    "encoding_size = 1\n",
    "word_idx = dict()\n",
    "for word, count in tqdm(word_count.items(), position=0):\n",
    "    if count >=100:\n",
    "        word_idx[word] = encoding_size\n",
    "        encoding_size += 1\n",
    "    else:\n",
    "        word_idx[word] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowa rzadkiej macierzy wektorów cech 'term-by-document'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8806/8806 [00:01<00:00, 5402.97it/s]\n"
     ]
    }
   ],
   "source": [
    "def encode(word_idx, word_count):\n",
    "    global encoding_size\n",
    "    encoding = np.zeros(encoding_size, dtype=np.uint16)\n",
    "    for word, count in word_count.items():\n",
    "        encoding[word_idx[word]] = count\n",
    "    return encoding\n",
    "        \n",
    "encodings = []\n",
    "for name in tqdm(documents, position=0):\n",
    "    encoding = encode(word_idx, bag_of_words[name])\n",
    "    encodings.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10033, 8806)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_by_document = np.stack(encodings, axis=1)\n",
    "term_by_document.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skalowanie macierzy cech\n",
    "Każdy element macierzy cech jest mnożony przez współczynnik 'inverse document frequency' celem redukcji znaczenia często występujących słów. np. słowo występujące we wszystkich tekstach zostanie pomnożone przez 0, czyli efektywnie będzie usunięte z rozważań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(N, term_by_document):\n",
    "    return np.log(N/np.sum(term_by_document != 0, axis=1, dtype=np.float32), \n",
    "                  dtype=np.float32)[:, None]\n",
    "N = len(documents)\n",
    "term_by_document_scaled =np.multiply(term_by_document, IDF(N, term_by_document), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10033, 8806)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_by_document_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poszukiwanie dokumentów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja `find(query, k)` wybiera k dokumentów najbardziej pasujących do zapytania `query`.  \n",
    "\n",
    "`query` jest napisem, który w funkcji jest rozkładany na pojedyncze słowa i kodowany do postaci wektorowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(query, k):\n",
    "    global term_by_document_scaled\n",
    "    global documents\n",
    "    global word_idx\n",
    "    query_words = split(query)\n",
    "    query_count = defaultdict(lambda: 0)\n",
    "    for word in query_words:\n",
    "        query_count[word] += 1\n",
    "    encoded_query = encode(word_idx, query_count)\n",
    "    probabilities = term_by_document_scaled.T @ encoded_query / (lin.norm(encoded_query) *\\\n",
    "                                                        lin.norm(term_by_document, axis=0))\n",
    "    idx = np.argsort(probabilities)\n",
    "   # print(max(probabilities))\n",
    "  #  print(probabilities[idx[-k:]])\n",
    "    return [documents[i] for i in idx[-k:]][::-1]                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = find(\"water\", 2)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = find(\"aloha united way\", 2)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = find(\"hospital cancer water\", 2)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacja wektorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(array):\n",
    "    if len(array.shape)==1:\n",
    "        return array / lin.norm(array)\n",
    "    else:\n",
    "        return array / lin.norm(array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_by_document_normalised = normalise(term_by_document_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poszukiwanie dokumentów z użyciem znormalizowanej macierzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_find(query, k):\n",
    "    global term_by_document_normalised\n",
    "    global documents\n",
    "    global word_idx\n",
    "    query_words = split(query)\n",
    "    query_count = defaultdict(lambda: 0)\n",
    "    for word in query_words:\n",
    "        query_count[word] += 1\n",
    "    encoded_query = normalise(encode(word_idx, query_count))\n",
    "    probabilities = term_by_document_normalised.T @ encoded_query \n",
    "    idx = np.argsort(probabilities)\n",
    "   # print(max(probabilities))\n",
    "  #  print(probabilities[idx[-k:]])\n",
    "    return [documents[i] for i in idx[-k:]][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = normalised_find(\"water\", 2)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = normalised_find(\"aloha united way\", 2)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = normalised_find(\"hospital cancer water\", 2)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usuwanie szumu przy uzyciu SVD ze znormalizowanej macierzy 'term by document' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = lin.svd(term_by_document_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = np.zeros((term_by_document_normalised.shape[0], term_by_document_normalised.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma[:term_by_document_normalised.shape[1], :term_by_document_normalised.shape[1]] = np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = term_by_document_normalised.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_by_document_aprrox = u @ Sigma[:,:k] @ vh[:k,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poszukiwanie dokumentów z użyciem odszumionej macierzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoised_find(query, results_num=3, k=1000):\n",
    "    global u, Sigma, vh\n",
    "    global documents\n",
    "    global word_idx\n",
    "    term_by_document_aprrox = u @ Sigma[:,:k] @ vh[:k,:]\n",
    "    query_words = split(query)\n",
    "    query_count = defaultdict(lambda: 0)\n",
    "    for word in query_words:\n",
    "        query_count[word] += 1\n",
    "    encoded_query = normalise(encode(word_idx, query_count))\n",
    "    probabilities = term_by_document_aprrox.T @ encoded_query \n",
    "    idx = np.argsort(probabilities)\n",
    "   # print(max(probabilities))\n",
    "   # print(probabilities[idx[-results_num:]])\n",
    "    return [documents[i] for i in idx[-results_num:]][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = denoised_find(\"aloha united way\", 2, 100)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = denoised_find(\"aloha united way\", 2, 1000)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = denoised_find(\"aloha united way\", 2, 6000)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = denoised_find(\"water\", 2, 6000)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_res = denoised_find(\"aloha united way\", 3, 8800)\n",
    "for filename in query_res:\n",
    "    with open(DATA_PATH + \"/\" +filename, \"r\") as file:\n",
    "        text = file.read().lower()\n",
    "    print(text)\n",
    "    print(((\"--\"*40)+\"\\n\")*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski dotyczące wartości k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wartość k ma bardzo duże znaczenie przy wykorzystaniu aproksymacji macierzy 'term by document'. Subiektywnie oceniając wyniki zwracane dla niskich wartości k (do 1000) są bezwartościowe i w ogóle niezwiązane z zapytaniem. Przy k równym kilku tysiącom wyniki są praktycznie nierozróżnialne od tych uzyskanych przy użyciu nieodszumionej macierzy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski dotyczące przekształcenia IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osobiście nie mogę stwierdzić, żeby wyniki zapytań znacząco się różniły w zależności od modyfikacji macierzy 'term by document'. Wyniki, które uzyskałem przy niezmodyfikowanej macierzy i macierzy przekształconej przez IDF są na tyle zbliżone, że różnica nie jest jednoznacznie możliwa do ocenienia subiektywnie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
